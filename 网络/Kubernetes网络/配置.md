# 配置

## ConfigMap

在[ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)中你可以选择配置以下：

- `debug` - 设置调试模式，更多的日志，`cilium-dbg monitor`也会展示更多内容。
- `enable-ipv4` - 启用IPv4
- `enable-ipv6` - 启用IPv6
- `clean-cilium-bpf-state` - 启动时删除文件系统中所有eBPF状态。Endpoint的IP不变，但是进行中的连接可能会有短暂的干扰，负载均衡决策也会丢失，因此经过负载均衡的活跃链接会中断。所有的eBPF状态会根据最初的来源（比如来自Kubernetes或kvstore）进行重建。这可以用来处理一些eBPF map方面遇到的严重问题。重启后这个选项就应该关掉了。
- `clean-cilium-state` - 删除**所有**Cilium的状态，包括那些无法找回的信息，比如所有endpoint的状态，以及一些可恢复的状态，比如已经被钉（pin）在文件系统、CNI配置文件、库代码、链接、路由等其它信息上的eBPF状态。**该操作无法撤销**。已存在的endpoint也许还会继续工作，但Cilium已经不管它们了，它们可能在没有任何警告信息的情况下就停止工作了。触发该操作后，endpoint必须断掉重连，让Cilium的新实例来接管。
- `monitor-aggregation` - 可以聚合`cilium-dbg monitor`中的跟踪事件，只对活动的流量，或者那些涉及到4层连接状态变更的数据包做周期性的更新。可选参数包括以下：
    - `none` - 为每个收发的数据包生成一个跟踪事件。
    - `low` - 为每个发送的数据包生成一个跟踪事件。
    - `medium` - 为每个新的连接生成一个跟踪事件，以及当数据包在它的方向上出现一个之前没有过的TCP flag时，以及每`monitor-aggregation-interval`生成一个跟踪事件（假设在一个间隔中至少有一个数据包出现）。每个方向上跟踪TCP标志并以各自的间隔分别上报。如果Cilium丢弃了某个数据包，那么每丢弃一个数据包就会发出一个事件。
    - `maxium` - 最大程度的聚合。目前其实等价于`medium`。
- `monitor-aggregation-interval` - 设置跟踪事件报告间隔。仅适用于`medium`及更高级别。假设每个间隔中都至少有一个新的数据包发出，那么可以确保平均每个间隔都有一个事件产生。
- `preallocate-bpf-maps` - 预分配的map映射，降低每个数据包的处理延迟，空间换时间。设置为`true`即可。如果这个值改了，下次Cilium启动的时候活跃链接可能会出现暂时的中断。

[ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)以及`cilium-etcd-secrets`的任何更改都需要重启Cilium。

> 注意，更新了ConfigMap中的配置后，最多需要2分钟才能将配置在整个集群传播开来。详见Kubernetes的[官方文档](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-automatically)。

下面这段ConfigMap配置中包含了双节点etcd，并且启用了TLS。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
data:
  # The kvstore configuration is used to enable use of a kvstore for state
  # storage.
  kvstore: etcd
  kvstore-opt: '{"etcd.config": "/var/lib/etcd-config/etcd.config"}'

  # This etcd-config contains the etcd endpoints of your cluster. If you use
  # TLS please make sure you follow the tutorial in https://cilium.link/etcd-config
  etcd-config: |-
    ---
    endpoints:
      - https://node-1:31079
      - https://node-2:31079
    #
    # In case you want to use TLS in etcd, uncomment the 'trusted-ca-file' line
    # and create a kubernetes secret by following the tutorial in
    # https://cilium.link/etcd-config
    trusted-ca-file: '/var/lib/etcd-secrets/etcd-client-ca.crt'
    #
    # In case you want client to server authentication, uncomment the following
    # lines and create a kubernetes secret by following the tutorial in
    # https://cilium.link/etcd-config
    key-file: '/var/lib/etcd-secrets/etcd-client.key'
    cert-file: '/var/lib/etcd-secrets/etcd-client.crt'

  # If you want to run cilium in debug mode change this value to true
  debug: "false"
  enable-ipv4: "true"
  # If you want to clean cilium state; change this value to true
  clean-cilium-state: "false"
```

### CNI

CNI就是Container Network Inteface的缩写，是Kubernetes的插件，用来委托网络配置。详见[CNI](https://github.com/containernetworking/cni)。

CNI的配置在Cilium部署的时候由DaemonSet自动处理。`cilium`Pod会生成恰当的CNI配置文件并写入磁盘。

> 注意，为了让CNI工作正常，`kubelet`必须要么运行在节点主机的文件系统上，要么就把`/etc/cni/net.d`和`/opt/cni/bin`挂载到`kubelet`运行的容器内。用Volume搞就行。

CNI的自动安装步骤如下：

1. `/etc/cni/net.d`和`/opt/cni/bin`目录从主机中挂载到Cilium的Pod中。
2. 将`cilium-cni`二进制安装到`/opt/cni/bin`中。覆盖重名文件。
3. 写入`/etc/cni/net.d/05-cilium.conflist`。

## 调整CNI配置

CNI的配置是由cilium的Pod自动生成并管理的。agent在初始化完成后就会写入该文件。而且agent默认会删除所有其他的CNI配置。

有很多Helm参数可以用来调整CNI配置。完整的去看helm的文档。这里是简单的：

Helm变量|描述|默认
-|-|-
`cni.customConf`|关闭CNI配置管理|false
`cni.customConf`|删除其他的CNI配置文件|true
`cni.install`|安装CNI配置及二进制|true

